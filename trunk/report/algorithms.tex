\section{Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchical Optimistic Optimization Algorithm}
$\mathcal{X}$-armed stuff here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Zooming Algorithm}
Zooming stuff here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discretization Algorithms}
To test the $\mathcal{X}$-armed and Zooming algorithms, we have implemented
the $\epsilon$-greedy, UCB1, and Exp3 finite-armed algorithms for
the case where whatever domain our bandit problem is over is discretized
into some finite number $K$ points.  In common to each algorithm is the
fact that the points are chosen randomly and uniformly from the domain.  The
intuition behind this choice is that, otherwise, it would be relatively easy
to construct a reward function that any discretization would do terribly
for, simply by ensuring that the peaks of the reward function did not occur
where the algorithm picked a point.  For example, if arms were chosen in
regular intervals from the interval [0,1], i.e. at the points 
$\frac{i}{K+1}$, with $1 \leq i \leq K$, then a reward function with a
sharp peak at 0 would be guaranteed to make any discretization algorithm
perform poorly.  Choosing the arms at random at least guarantees that there
is a chance of doing well.  A brief description of the particulars of 
each algorithm is now given.

\subsubsection{$\epsilon$-Greedy}
In round $t$, if $1 \leq t \leq K$ then play arm $t$.  Otherwise, play
randomly with probability $\epsilon$ and the arm with the best running
average otherwise.  We have implemented this algorithm using
$\epsilon = \frac{K}{t}$.

\subsubsection{UCB1}
In round $t$, if $1 \leq i \leq K$ then play arm $i$.  Otherwise, play
the arm that maximizes the quantity
\[
	\bar{r_i} + \sigma \sqrt{\frac{2 \log(t)}{p_i}}
\]
Where $\bar{r_i}$ is the average reward of arm $i$ so far, $p_i$ is the 
number of times arm $i$ has been played so far, and $\sigma$ is a parameter
that can be chosen to influence how much exploration is done.

\subsubsection{Exp3}
Initialize $w_i(1) = 1$ for all $1 \leq i \leq K$.  In round $t$, play
arm $i$ with probability
\[
	p_i(t) = \frac{\epsilon}{K} + (1 - \epsilon) \frac{w_i(t)}{\sum_j w_j(t)}
\]
With $i$ the arm that was played, update
\[
	w_i(t+1) = w_i(t)\gamma^{\frac{\epsilon r_i(t)}{K p_i(t)}}
\]
where $r_i(t)$ is the reward recieved, and set $w_j(t+1) = w_j(t)$ for all
other arms $j$.  $\gamma$ is a parameter that can be chosen to influence how
much exploration is done.  As in the $\epsilon$-greedy algorithm, we use
$\epsilon = \frac{K}{t}$.